{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using Recurrent Models\n",
    "\n",
    "This notebook compares the performance of RNN, LSTM, GRU, and BiLSTM for sentiment analysis using the IMDB dataset. Each model is trained for 10 epochs with fixed units, using the Adam optimizer.\n",
    "\n",
    "## Overview\n",
    "The key steps involve importing the dataset, defining and training various recurrent models, and comparing their performance.\n",
    "\n",
    "## Procedure\n",
    "- **Dataset Preparation**: Import the IMDB dataset and convert it to vector form using the Bag of Words technique.\n",
    "- **RNN Model**: Define and train an RNN model on the dataset.\n",
    "- **LSTM Model**: Define and train an LSTM model on the dataset.\n",
    "- **GRU Model**: Define and train a GRU model on the dataset.\n",
    "- **BiLSTM Model**: Define and train a BiLSTM model on the dataset.\n",
    "- **Performance Comparison**: Compare the accuracy of all models to determine the best performer.\n",
    "\n",
    "References:\n",
    "- [IMDB Dataset of 50K Movie Reviews](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically there's a family where a little boy ...          0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"imdb.csv\", usecols=[\"review\", \"sentiment\"], encoding='latin-1')\n",
    "## 1 - positive, 0 - negative\n",
    "df.sentiment = (df.sentiment == \"positive\").astype(\"int\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000 7499 7499\n",
      "35000 35000 7499\n"
     ]
    }
   ],
   "source": [
    "val_size = int(df.shape[0] * 0.15)\n",
    "test_size = int(df.shape[0] * 0.15)\n",
    "\n",
    "\n",
    "def train_val_test_split(df=None, train_percent=0.7, test_percent=0.15, val_percent=0.15):\n",
    "  df = df.sample(frac=1)\n",
    "  train_df = df[: int(len(df)*train_percent)]\n",
    "  test_df = df[int(len(df)*train_percent)+1 : int(len(df)*(train_percent+test_percent))]\n",
    "  val_df = df[int(len(df)*(train_percent + test_percent))+1 : ]\n",
    "  return train_df, test_df, val_df\n",
    "\n",
    "train_df, test_df, val_df = train_val_test_split(df, 0.7, 0.15, 0.15)\n",
    "train_labels, train_texts = train_df.values[:,1], train_df.values[:,0]\n",
    "val_labels, val_texts = val_df.values[:,1], val_df.values[:,0]\n",
    "test_labels, test_texts = test_df.values[:,1], test_df.values[:,0]\n",
    "print(len(train_df), len(test_df), len(val_df))\n",
    "print(len(train_texts), len(train_labels), len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "\n",
    "def process_tokens(text):\n",
    "    \"\"\"\n",
    "    function to process tokens, replace any unwanted chars\n",
    "    \"\"\"\n",
    "    preprocessed_text = text.lower().replace(\",\", \"\").replace(\".\", \"\").replace(\":\", \"\").replace(\")\", \"\").replace(\"-\", \"\").replace(\"(\", \"\")\n",
    "    preprocessed_text = ''.join([i for i in preprocessed_text if not preprocessed_text.isdigit()])\n",
    "    return preprocessed_text\n",
    "\n",
    "def preprocessing(data):\n",
    "    \"\"\"\n",
    "    preprocessing data to list of tokens\n",
    "    \"\"\"\n",
    "    nlp = English()\n",
    "    tokenizer = Tokenizer(nlp.vocab)\n",
    "    preprocessed_data = []\n",
    "    for sentence in data:\n",
    "        sentence = process_tokens(sentence)\n",
    "        tokens = tokenizer(sentence)\n",
    "        tlist = []\n",
    "        for token in tokens:\n",
    "            tlist.append(str(token))\n",
    "        preprocessed_data.append(tlist)\n",
    "    return preprocessed_data\n",
    "\n",
    "train_data = preprocessing(train_texts)\n",
    "val_data = preprocessing(val_texts)\n",
    "test_data = preprocessing(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "## Creating a vectorizer to vectorize text and create matrix of features\n",
    "## Bag of words technique\n",
    "class Vectorizer():\n",
    "    def __init__(self, max_features):\n",
    "        self.max_features = max_features\n",
    "        self.vocab_list = None\n",
    "        self.token_to_index = None\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        word_dict = {}\n",
    "        for sentence in dataset:\n",
    "            for token in sentence:\n",
    "                if token not in word_dict:\n",
    "                    word_dict[token] = 1\n",
    "                else:\n",
    "                    word_dict[token] += 1\n",
    "        word_dict = dict(sorted(word_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "        end_to_slice = min(len(word_dict), self.max_features)\n",
    "        word_dict = dict(itertools.islice(word_dict.items(), end_to_slice))\n",
    "        self.vocab_list = list(word_dict.keys())\n",
    "        self.token_to_index = {}\n",
    "        counter = 0\n",
    "        for token in self.vocab_list:\n",
    "            self.token_to_index[token] = counter\n",
    "            counter += 1\n",
    "\n",
    "\n",
    "    def transform(self, dataset):\n",
    "        data_matrix = np.zeros((len(dataset), len(self.vocab_list)))\n",
    "        for i, sentence in enumerate(dataset):\n",
    "            for token in sentence:\n",
    "                if token in self.token_to_index:\n",
    "                    data_matrix[i, self.token_to_index[token]] += 1\n",
    "        return data_matrix\n",
    "\n",
    "## max features - top k words to consider only\n",
    "max_features = 2000\n",
    "\n",
    "vectorizer = Vectorizer(max_features=max_features)\n",
    "vectorizer.fit(train_data)\n",
    "\n",
    "## Checking if the len of vocab = k\n",
    "X_train = vectorizer.transform(train_data)\n",
    "X_val = vectorizer.transform(val_data)\n",
    "X_test = vectorizer.transform(test_data)\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)\n",
    "y_test = np.array(test_labels)\n",
    "\n",
    "vocab = vectorizer.vocab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (35000, 1, 2000), y_train.shape: (35000, 2)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "keras = tf.keras\n",
    "from keras.utils import to_categorical\n",
    "y_train = y_train.astype('int')\n",
    "y_val = y_val.astype('int')\n",
    "y_test = y_test.astype('int')\n",
    "\n",
    "y_train = to_categorical(y_train, 2)\n",
    "y_test = to_categorical(y_test, 2)\n",
    "y_val = to_categorical(y_val, 2)\n",
    "\n",
    "X_train = X_train.reshape(-1, 1, X_train.shape[1])\n",
    "X_val = X_val.reshape(-1, 1, X_val.shape[1])\n",
    "X_test = X_test.reshape(-1, 1, X_test.shape[1])\n",
    "\n",
    "y_train = y_train.reshape(-1, 2)\n",
    "y_val = y_val.reshape(-1, 2)\n",
    "y_test = y_test.reshape(-1, 2)\n",
    "\n",
    "print(f'X_train.shape: {X_train.shape}, y_train.shape: {y_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters for all models\n",
    "BATCH_SIZE = 256\n",
    "LR = 0.01\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 256)               577792    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 578306 (2.21 MB)\n",
      "Trainable params: 578306 (2.21 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "137/137 [==============================] - 5s 25ms/step - loss: 0.5193 - accuracy: 0.8097 - val_loss: 0.3040 - val_accuracy: 0.8751\n",
      "Epoch 2/10\n",
      "137/137 [==============================] - 3s 21ms/step - loss: 0.2686 - accuracy: 0.8873 - val_loss: 0.3036 - val_accuracy: 0.8736\n",
      "Epoch 3/10\n",
      "137/137 [==============================] - 3s 21ms/step - loss: 0.2262 - accuracy: 0.9069 - val_loss: 0.3108 - val_accuracy: 0.8713\n",
      "Epoch 4/10\n",
      "137/137 [==============================] - 3s 22ms/step - loss: 0.1725 - accuracy: 0.9323 - val_loss: 0.3469 - val_accuracy: 0.8690\n",
      "Epoch 5/10\n",
      "137/137 [==============================] - 3s 20ms/step - loss: 0.1167 - accuracy: 0.9589 - val_loss: 0.3889 - val_accuracy: 0.8662\n",
      "Epoch 6/10\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.0684 - accuracy: 0.9800 - val_loss: 0.4306 - val_accuracy: 0.8624\n",
      "Epoch 7/10\n",
      "137/137 [==============================] - 3s 20ms/step - loss: 0.0345 - accuracy: 0.9922 - val_loss: 0.4652 - val_accuracy: 0.8697\n",
      "Epoch 8/10\n",
      "137/137 [==============================] - 3s 20ms/step - loss: 0.0183 - accuracy: 0.9964 - val_loss: 0.5023 - val_accuracy: 0.8662\n",
      "Epoch 9/10\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.0106 - accuracy: 0.9982 - val_loss: 0.5414 - val_accuracy: 0.8666\n",
      "Epoch 10/10\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.0072 - accuracy: 0.9987 - val_loss: 0.5578 - val_accuracy: 0.8664\n",
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
      "Test loss: 0.5322189331054688\n",
      "Test accuracy: 0.8718495965003967\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN, Dropout\n",
    "from keras.optimizers.legacy import Adam\n",
    "\n",
    "\n",
    "model = None\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(256, input_shape=(1, max_features)))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "optimizer = Adam(learning_rate=LR)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, \n",
    "              metrics=['accuracy'], )\n",
    "print(model.summary())\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    history = model.fit(X_train, y_train,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=EPOCHS)\n",
    "print(history.history.keys())\n",
    "\n",
    "score, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "model_results['RNN'] = {\n",
    "    'loss': score,\n",
    "    'accuracy': acc,\n",
    "    'history': history\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_10 (LSTM)              (None, 256)               2311168   \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2311682 (8.82 MB)\n",
      "Trainable params: 2311682 (8.82 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "137/137 [==============================] - 5s 27ms/step - loss: 0.3793 - accuracy: 0.8275 - val_loss: 0.3018 - val_accuracy: 0.8717\n",
      "Epoch 2/10\n",
      "137/137 [==============================] - 3s 22ms/step - loss: 0.2616 - accuracy: 0.8924 - val_loss: 0.2934 - val_accuracy: 0.8787\n",
      "Epoch 3/10\n",
      "137/137 [==============================] - 3s 21ms/step - loss: 0.2128 - accuracy: 0.9157 - val_loss: 0.2974 - val_accuracy: 0.8776\n",
      "Epoch 4/10\n",
      "137/137 [==============================] - 3s 22ms/step - loss: 0.1558 - accuracy: 0.9403 - val_loss: 0.3180 - val_accuracy: 0.8771\n",
      "Epoch 5/10\n",
      "137/137 [==============================] - 3s 22ms/step - loss: 0.1067 - accuracy: 0.9620 - val_loss: 0.3680 - val_accuracy: 0.8746\n",
      "Epoch 6/10\n",
      "137/137 [==============================] - 3s 22ms/step - loss: 0.0649 - accuracy: 0.9803 - val_loss: 0.3992 - val_accuracy: 0.8684\n",
      "Epoch 7/10\n",
      "137/137 [==============================] - 3s 21ms/step - loss: 0.0387 - accuracy: 0.9900 - val_loss: 0.4429 - val_accuracy: 0.8745\n",
      "Epoch 8/10\n",
      "137/137 [==============================] - 3s 20ms/step - loss: 0.0220 - accuracy: 0.9950 - val_loss: 0.4889 - val_accuracy: 0.8726\n",
      "Epoch 9/10\n",
      "137/137 [==============================] - 3s 21ms/step - loss: 0.0130 - accuracy: 0.9973 - val_loss: 0.5074 - val_accuracy: 0.8752\n",
      "Epoch 10/10\n",
      "137/137 [==============================] - 3s 20ms/step - loss: 0.0084 - accuracy: 0.9983 - val_loss: 0.5330 - val_accuracy: 0.8724\n",
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
      "Test loss: 0.55006\n",
      "Test accuracy: 0.86732\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "model = None\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(1, max_features)))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "optimizer = Adam(learning_rate=LR)\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=optimizer, \n",
    "     metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    history = model.fit(X_train, y_train,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=EPOCHS)\n",
    "print(history.history.keys())\n",
    "\n",
    "score, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss: {:.5f}'.format(score))\n",
    "print('Test accuracy: {:.5f}'.format(acc))\n",
    "\n",
    "model_results['LSTM'] = {\n",
    "    'loss': score,\n",
    "    'accuracy': acc,\n",
    "    'history': history,\n",
    "    'model': model\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_1 (GRU)                 (None, 256)               1734144   \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1734658 (6.62 MB)\n",
      "Trainable params: 1734658 (6.62 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "137/137 [==============================] - 6s 26ms/step - loss: 0.4317 - accuracy: 0.8124 - val_loss: 0.2936 - val_accuracy: 0.8764\n",
      "Epoch 2/10\n",
      "137/137 [==============================] - 3s 20ms/step - loss: 0.2703 - accuracy: 0.8869 - val_loss: 0.3003 - val_accuracy: 0.8720\n",
      "Epoch 3/10\n",
      "137/137 [==============================] - 3s 20ms/step - loss: 0.2327 - accuracy: 0.9041 - val_loss: 0.3027 - val_accuracy: 0.8763\n",
      "Epoch 4/10\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.1904 - accuracy: 0.9238 - val_loss: 0.3164 - val_accuracy: 0.8775\n",
      "Epoch 5/10\n",
      "137/137 [==============================] - 3s 20ms/step - loss: 0.1404 - accuracy: 0.9463 - val_loss: 0.3429 - val_accuracy: 0.8708\n",
      "Epoch 6/10\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.0933 - accuracy: 0.9676 - val_loss: 0.3984 - val_accuracy: 0.8720\n",
      "Epoch 7/10\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.0568 - accuracy: 0.9823 - val_loss: 0.4401 - val_accuracy: 0.8696\n",
      "Epoch 8/10\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.0342 - accuracy: 0.9912 - val_loss: 0.4847 - val_accuracy: 0.8681\n",
      "Epoch 9/10\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.0212 - accuracy: 0.9955 - val_loss: 0.5283 - val_accuracy: 0.8686\n",
      "Epoch 10/10\n",
      "137/137 [==============================] - 3s 20ms/step - loss: 0.0138 - accuracy: 0.9973 - val_loss: 0.5589 - val_accuracy: 0.8698\n",
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
      "Test loss: 0.5596\n",
      "Test accuracy: 0.8685\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GRU\n",
    "\n",
    "model = None\n",
    "model = Sequential()\n",
    "model.add(GRU(256, input_shape=(1, max_features)))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "optimizer = Adam(learning_rate=LR)\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=optimizer, \n",
    "     metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    history = model.fit(X_train, y_train,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=EPOCHS)\n",
    "    \n",
    "print(history.history.keys())\n",
    "\n",
    "score, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss: {:.4f}'.format(score))\n",
    "print('Test accuracy: {:.4f}'.format(acc))\n",
    "\n",
    "model_results['GRU'] = {\n",
    "    'loss': score,\n",
    "    'accuracy': acc,\n",
    "    'history': history,\n",
    "    'model': model\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_8 (Bidirecti  (None, 512)               4622336   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4623362 (17.64 MB)\n",
      "Trainable params: 4623362 (17.64 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "137/137 [==============================] - 7s 36ms/step - loss: 0.3929 - accuracy: 0.8232 - val_loss: 0.2919 - val_accuracy: 0.8779\n",
      "Epoch 2/10\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.2698 - accuracy: 0.8860 - val_loss: 0.2912 - val_accuracy: 0.8764\n",
      "Epoch 3/10\n",
      "137/137 [==============================] - 4s 30ms/step - loss: 0.2299 - accuracy: 0.9032 - val_loss: 0.2965 - val_accuracy: 0.8759\n",
      "Epoch 4/10\n",
      "137/137 [==============================] - 4s 29ms/step - loss: 0.1781 - accuracy: 0.9295 - val_loss: 0.3140 - val_accuracy: 0.8757\n",
      "Epoch 5/10\n",
      "137/137 [==============================] - 4s 30ms/step - loss: 0.1165 - accuracy: 0.9576 - val_loss: 0.3654 - val_accuracy: 0.8721\n",
      "Epoch 6/10\n",
      "137/137 [==============================] - 4s 27ms/step - loss: 0.0696 - accuracy: 0.9778 - val_loss: 0.3983 - val_accuracy: 0.8682\n",
      "Epoch 7/10\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.0358 - accuracy: 0.9908 - val_loss: 0.4513 - val_accuracy: 0.8737\n",
      "Epoch 8/10\n",
      "137/137 [==============================] - 4s 29ms/step - loss: 0.0165 - accuracy: 0.9966 - val_loss: 0.4868 - val_accuracy: 0.8737\n",
      "Epoch 9/10\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.0091 - accuracy: 0.9983 - val_loss: 0.5267 - val_accuracy: 0.8701\n",
      "Epoch 10/10\n",
      "137/137 [==============================] - 4s 27ms/step - loss: 0.0054 - accuracy: 0.9991 - val_loss: 0.5547 - val_accuracy: 0.8738\n",
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
      "Test loss: 0.5759\n",
      "Test accuracy: 0.8702\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional\n",
    "\n",
    "model = None\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(256), input_shape=(1, max_features)))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "optimizer = Adam(learning_rate=LR)\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=optimizer, \n",
    "     metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    history = model.fit(X_train, y_train,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=EPOCHS)\n",
    "    \n",
    "print(history.history.keys())\n",
    "\n",
    "score, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss: {:.4f}'.format(score))\n",
    "print('Test accuracy: {:.4f}'.format(acc))\n",
    "\n",
    "model_results['BiLSTM'] = {\n",
    "    'loss': score,\n",
    "    'accuracy': acc,\n",
    "    'history': history,\n",
    "    'model': model\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN    - Test loss: 0.5377, Test accuracy: 0.8690\n",
      "LSTM   - Test loss: 0.5501, Test accuracy: 0.8673\n",
      "GRU    - Test loss: 0.5596, Test accuracy: 0.8685\n",
      "BiLSTM - Test loss: 0.5759, Test accuracy: 0.8702\n"
     ]
    }
   ],
   "source": [
    "%precision %.4f\n",
    "for model_type, results in model_results.items():\n",
    "    print(f'{model_type:6s} - Test loss: {results[\"loss\"]:.4f}, Test accuracy: {results[\"accuracy\"]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the performance of all the models. In which case do you get the best accuracy?\n",
    "\n",
    "``` text\n",
    "RNN    - Test loss: 0.5377, Test accuracy: 0.8690\n",
    "LSTM   - Test loss: 0.5501, Test accuracy: 0.8673\n",
    "GRU    - Test loss: 0.5596, Test accuracy: 0.8685\n",
    "BiLSTM - Test loss: 0.5759, Test accuracy: 0.8702\n",
    "```\n",
    "\n",
    "The BiLSTM model outperformed the other models with a test accuracy of 0.8702. This is likely due to its ability to capture contextual information from both forward and backward directions of the input sequence. The RNN performed the worst with a test accuracy of 0.8690, likely due to its inability to capture long-term dependencies in the input sequence compared to LSTM, GRU, and BiLSTM. The LSTM and GRU models performed similarly, with test accuracies of 0.8673 and 0.8685."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MetalTF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
