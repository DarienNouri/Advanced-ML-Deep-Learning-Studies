{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Activation Function\n",
    "\n",
    "This notebook explores the properties of the softmax activation function and its derivatives, as well as the use of the cross-entropy loss function.\n",
    "\n",
    "## Overview\n",
    "The key steps involve proving the derivatives of the softmax function and showing the correctness of the gradient of the cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Activation Function\n",
    "\n",
    "$$\n",
    "\\sigma_{o_j} = \\frac{e^{o_j}}{\\sum_{k=1}^{K}e^{o_k}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "For both $i=j$ and $i \\neq j$ cases, we will use the quotient rule. \n",
    "\n",
    "\n",
    "For $i = j$: \n",
    "\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\sigma_{v_j}}{\\partial v_i}  \n",
    "        &= \\frac{ e^{v_j} \\sum_{j=1}^{k} e^{v_j} - e^{v_j}e^{v_i}}{(\\sum_{j=1}^{k} e^{v_j})^2} \\\\\n",
    "        &= \\frac{ e^{v_j}}{\\sum_{j=1}^{k} e^{v_j}} - \\left(\\frac{ e^{v_j}}{\\sum_{j=1}^{k} e^{v_j}}\\right)^2  \\\\ \n",
    "        &= \\sigma_{v_j} - \\sigma_{v_j}^2 \\\\ \n",
    "        &= \\sigma_{v_j}(1 - \\sigma_{v_j})\n",
    "\\end{align*} \n",
    "\n",
    "\n",
    "For $i \\neq j$:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\sigma_{v_j}}{\\partial v_i}  \n",
    "        &= \\frac{- e^{v_j}e^{v_i}}{(\\sum_{j=1}^{k} e^{v_j})^2} \\\\\n",
    "        &= - \\sigma_{v_j}\\sigma_{v_i}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "We want to find the gradient of the loss function with respect to the input $v_i$, which is $\\frac{\\partial L}{\\partial v_i}$.\n",
    "\n",
    "To compute $\\frac{\\partial L}{\\partial v_i}$, we use the chain rule, summing over all classes because the output $o_i$ for each class depends on all inputs $v_j$: $$ \\frac{\\partial L}{\\partial v_i} = \\sum_{j=1}^{k} \\frac{\\partial L}{\\partial o_j} \\frac{\\partial o_j}{\\partial v_i} $$\n",
    "\n",
    "first lets find the derivative of the loss w.r.t the softmax function:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial o_i} = -\\frac{y_i}{o_i}\n",
    "$$\n",
    "\n",
    "\n",
    "Substituting $\\frac{\\partial L}{\\partial o_j} = -\\frac{y_j}{o_j}$ and using the results from part 1:\n",
    "\n",
    "\n",
    "* For $j = i$, $\\frac{\\partial o_j}{\\partial v_i} = o_i(1 - o_i)$\n",
    "\n",
    "* For $j \\neq i$, $\\frac{\\partial o_j}{\\partial v_i} = -o_io_j$\n",
    "\n",
    "We get: $$ \\frac{\\partial L}{\\partial v_i} = -\\frac{y_i}{o_i} o_i(1 - o_i) + \\sum_{j \\neq i} -\\frac{y_j}{o_j} (-o_io_j) $$ $$ = -y_i(1 - o_i) + \\sum_{j \\neq i} y_jo_i $$ $$ = -y_i + y_io_i + o_i\\sum_{j \\neq i} y_j $$\n",
    "\n",
    "Since $y_i$ is one-hot encoded, $\\sum_{j=1}^{k} y_j = 1$, and $\\sum_{j \\neq i} y_j = 1 - y_i$. Substituting this in gives: $$ \\frac{\\partial L}{\\partial v_i} = -y_i + y_io_i + o_i(1 - y_i) $$ $$ = o_i - y_i $$.\n",
    "\n",
    "This shows the correctness of the equation $\\frac{\\partial L}{\\partial v_i} = o_i - y_i$ using the results from part 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EnvML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
