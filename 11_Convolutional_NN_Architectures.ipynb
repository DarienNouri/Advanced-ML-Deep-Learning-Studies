{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcDhlfqyBd6m"
      },
      "source": [
        "# Convolutional Neural Networks Architectures\n",
        "\n",
        "This notebook studies and compares different convolutional neural network architectures. It includes calculating the number of parameters, memory requirements, and analyzing inception modules.\n",
        "\n",
        "## Overview\n",
        "The key steps involve calculating parameters for AlexNet and VGG19, understanding inception modules, and comparing architectures.\n",
        "\n",
        "## Procedure\n",
        "- **AlexNet Parameters**: Calculate the number of parameters for each layer of AlexNet and sum them.\n",
        "- **VGG19 Parameters**: Complete Table 1 for VGG19, calculating activation units and parameters for each layer.\n",
        "- **Receptive Field Calculation**: Show that a stack of N convolution layers with filter size F×F has the same receptive field as one convolution layer with filter size (NF - N + 1) × (NF - N + 1). Calculate the receptive field for 3 filters of size 5x5.\n",
        "- **Inception Module Analysis**:\n",
        "  - General idea behind inception modules.\n",
        "  - Calculate output size after each filter and filter concatenation for naive and dimensionality reduction architectures.\n",
        "  - Calculate the number of convolutional operations for each inception architecture.\n",
        "  - Explain the computational complexity and savings of dimensionality reduction architecture.\n",
        "- **Faster R-CNN**:\n",
        "  - Main difference between Fast R-CNN and Faster R-CNN.\n",
        "  - Explain the architecture of Region Proposal Network (RPN).\n",
        "  - Describe how region proposals are generated from RPN.\n",
        "  - Technique to reduce the number of proposals and its working.\n",
        "\n",
        "References:\n",
        "- [AlexNet Paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n",
        "- [VGG Paper](https://arxiv.org/pdf/1409.1556.pdf)\n",
        "- [GoogLeNet Paper](https://arxiv.org/pdf/1409.484​⬤"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-id00ye6CNLB"
      },
      "source": [
        "## 1.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EfcEgq1RSQv"
      },
      "source": [
        "| Layer    | Output Size  | Strides | Padding | Weight          | Biases | Parameters   |\n",
        "|:--------:|:------------:|:-------:|:-------:|:---------------:|:------:|:------------:|\n",
        "| Input    | 227*227*3    |   -     |   -     |       0         |   0    |      0       |\n",
        "| Conv-1   | 55*55*96     |   4     |   2     | 34,848          |   96   | 34,944       |\n",
        "| MaxPool-1| 27*27*96     |   2     |   0     |       0         |   0    |      0       |\n",
        "| Conv-2   | 27*27*256    |   1     |   2     | 614,400         |  256   | 614,656      |\n",
        "| MaxPool-2| 13*13*256    |   2     |   0     |       0         |   0    |      0       |\n",
        "| Conv-3   | 13*13*384    |   1     |   1     | 884,736         |  384   | 885,120      |\n",
        "| Conv-4   | 13*13*384    |   1     |   1     | 1,327,104       |  384   | 1,327,488    |\n",
        "| Conv-5   | 13*13*256    |   1     |   1     | 884,992         |  256   | 885,248      |\n",
        "| MaxPool-3| 6*6*256      |   2     |   0     |       0         |   0    |      0       |\n",
        "| FC-1     | 4096         |   -     |   -     | 37,748,736      | 4096   | 37,752,832   |\n",
        "| FC-2     | 4096         |   -     |   -     | 16,777,216      | 4096   | 16,781,312   |\n",
        "| FC-3     | 1000         |   -     |   -     | 4,096,000       | 1000   | 4,097,000    |\n",
        "| Output   | 1000         |   -     |   -     |       0         |   0    |      0       |\n",
        "| **Total**|              |         |         | **62,378,344**   |       |              |\n",
        "\n",
        "Total Parameters for alexnet = $$34,944 + 614,656 + 885,120 + 1,327,488  + \\\\ 885,248 + 37,752,832  + 16,781,312 + 4,097,000 = 62,378,344$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1D_yfneCWqL"
      },
      "source": [
        "## 1.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6Y1ZWsFVy27"
      },
      "source": [
        "\n",
        "Layer | Number of Activations (Memory) | Parameters (Compute)\n",
        ":-----:|:----------:|:---:\n",
        "Input | 224\\*224\\*3 = 150K | 0\n",
        "CONV3-64 | 224\\*224\\*64 = 3.2M | (3\\*3\\*3)\\*64 = 1,728\n",
        "CONV3-64 | 224\\*224\\*64 = 3.2M | (3\\*3\\*64)\\*64 = 36,864\n",
        "POOL2 |  112\\*112\\*64 = 800K | 0\n",
        "CONV3-128 | 112\\*112\\*128 = 1.6M | (3\\*3\\*64)\\*128 = 73,728\n",
        "CONV3-128 | 112\\*112\\*128 = 1.6M | (3\\*3\\*128)\\*128 = 147,456\n",
        "POOL2 |  56\\*56\\*128 = 400K | 0\n",
        "CONV3-256 | 56\\*56\\*256 = 800K | (3\\*3\\*128)\\*256 = 294,912\n",
        "CONV3-256 | 56\\*56\\*256 = 800K | (3\\*3\\*256)\\*256 = 589,824\n",
        "CONV3-256 | 56\\*56\\*256 = 800K | (3\\*3\\*256)\\*256 = 589,824\n",
        "CONV3-256 | 56\\*56\\*256 = 800K | (3\\*3\\*256)\\*256 = 589,824\n",
        "POOL2 | 28\\*28\\*256 = 200K | 0\n",
        "CONV3-512 | 28\\*28\\*512 = 400K | (3\\*3\\*256)\\*512 = 1,179,648\n",
        "CONV3-512 | 28\\*28\\*512 = 400K | (3\\*3\\*512)\\*512 = 2,359,296\n",
        "CONV3-512 | 28\\*28\\*512 = 400K | (3\\*3\\*512)\\*512 = 2,359,296\n",
        "CONV3-512 | 28\\*28\\*512 = 400K | (3\\*3\\*512)\\*512 = 2,359,296\n",
        "POOL2 | 14\\*14\\*512 = 100K | 0\n",
        "CONV3-512 | 14\\*14\\*512 = 100K | (3\\*3\\*512)\\*512 = 2,359,296\n",
        "CONV3-512 | 14\\*14\\*512 = 100K | (3\\*3\\*512)\\*512 = 2,359,296\n",
        "CONV3-512 | 14\\*14\\*512 = 100K | (3\\*3\\*512)\\*512 = 2,359,296\n",
        "CONV3-512 | 14\\*14\\*512 = 100K | (3\\*3\\*512)\\*512 = 2,359,296\n",
        "POOL2 | 7\\*7\\*512 = 25K | 0\n",
        "FC | 4096 | -\n",
        "FC | 4096 | 4096\\*4096 = 16,777,216\n",
        "FC | 1000 | -  \n",
        "**Total** | **143.7M** | **143.67M**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYpokEaLYBwY"
      },
      "source": [
        "## 1.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "A single convolution with a filter of size (NF - N + 1)x(NF - N + 1) can cover the same receptive field as N convolutional layers of filter size FxF.\n",
        "    Imagine stacking the layers. Each layer increases the receptive field by F-1.\n",
        "    After N layers, the total increase is N(F-1) resulting in a receptive field of F + N(F-1) = NF - N + 1\n",
        "\n",
        "Calculating Receptive Field for 3x 5x5 Filters:\n",
        "\n",
        "* F = 5\n",
        "* N = 3\n",
        "* Receptive Field = (3 * 5) - 3 + 1 = 13x13\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWZHh02CYKKY"
      },
      "source": [
        "## 1.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3wfpKtdYMIo"
      },
      "source": [
        "### a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhQdXFq-YT0-"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "The general idea behind designing an inception module in a convolutional neural network is because inception modules allow for more efficient computations and deeper networks through a dimensionality reduction with stacked 1×1 convolutions. Inception modules were designed to help ease computational expenses and overfitting as a bigger model is more prone to overfitting and increasing parameters will result in more resources.Thus, the solution is to take multiple kernel filter sizes within the convoluted neural network and ordering them to operate on the same level instead of stacking them sequentially. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK87FU3SYUlL"
      },
      "source": [
        "### b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Naive filter concatenation results in dimensions of 32x32x672, combining 1x1, 3x3, 5x5 convolutions and 3x3 max pooling, which halves spatial dimensions.\n",
        "\n",
        "In contrast, dimension reduction leads to 32x32x896 dimensions due to steps including two 1x1 convolutions(128), 3x3 convolutions(192), additional 1x1 convolutions(32 and 64), 5x5 convolutions(96), and 3x3 max pooling that also halves spatial dimensions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jG-3Cc4YUsu"
      },
      "source": [
        "### c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the naive version, total operations are 1,115,684,176, calculated as follows: 1x1 convolutions (128) require 1,048,576 operations, 3x3 convolutions (192) need 530,841,600 operations, and 5x5 convolutions (96) need 1,179,648,000 operations. 3x3 max pooling doesn't add extra operations.\n",
        "\n",
        "In dimension reduction, the total operations are 1,115,472,184. Breakdown: two 1x1 convolutions (128) require 1,048,576 operations each, 3x3 convolutions (192) need 530,841,600 operations, another 1x1 convolutions (32) require 262,144 operations, 5x5 convolutions (96) need 1,179,648,000 operations, and a final 1x1 convolutions (64) needs 524,288 operations. 3x3 max pooling adds no extra operations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyC53m8gYUzm"
      },
      "source": [
        "### d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The naive architecture's high computation needs slow training and increase costs. However, the dimension reduction architecture reduces this load by introducing 1x1 convolution filters before larger ones. This approach maintains feature capturing while boosting computational efficiency.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt7M2Fs_YUzn"
      },
      "source": [
        "\n",
        "The naive architecture is problematic due to its high computational complexity, necessitating a substantial number of convolutional operations. This results in computational expenses and slow training. On the other hand, the dimensionality reduction architecture offers a solution by reducing the computational load. It achieves this by first employing 1x1 convolution filters to reduce the number of channels before applying larger filters like 3x3 and 5x5. This strategic approach enhances computational efficiency without compromising the model's ability to capture intricate features.a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9XYBTwQYcJy"
      },
      "source": [
        "## 1.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb-CINl5YcJz"
      },
      "source": [
        "### a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Faster-RCNN accelerates detection compared to Fast-RCNN by revamping region proposal generation. Fast-RCNN employs an external region proposal methodology, such as Selective Search, which is computation-heavy, thus reducing detection speed. However, Faster-RCNN incorporates a Region Proposal Network (RPN) within the CNN architecture, enabling convolutional feature sharing with the detection network. This makes region proposal more streamlined and efficient, boosting object detection speed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJ7NiQajYcJz"
      },
      "source": [
        "### b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Faster-RCNN generates region proposals. Features: base convolutional network (often pre-trained on ImageNet), several subsequent convolutional layers. Base network predicts object bounding boxes and objectness scores at each feature map position. RPN uses varied anchor boxes for proposal prediction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWRyaZiMYcJz"
      },
      "source": [
        "### c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RPN scans a feature map with anchor boxes to predict potential object bounding boxes. Each anchor box generates coordinates and an objectness score, indicating the probability of containing an object. High-scoring proposals are selected as region proposals. These proposals, along with their objectness scores, are used in subsequent object detection stages.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuu8dqGUYcJz"
      },
      "source": [
        "### d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NMS is used in Faster-RCNN to reduce overlapping region proposals from RPN. It selects the proposal with the highest objectness score and discards any other proposals that significantly overlap with it (measured using IoU). This process continues until the desired number of proposals is reached.\n",
        "\n",
        "\n",
        "In an image with overlapping proposals, NMS selects the proposal with the highest objectness score and removes any others that overlap significantly. This ensures that only the most promising proposals are used in subsequent object detection stages. In the image below, the green boxes represent the proposals selected by NMS.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
