{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parallelism in PyTorch\n",
    "\n",
    "This notebook experiments with PyTorch’s DataParallel Module, which implements Synchronous SGD across multiple GPUs. It involves training a ResNet-18 model on the CIFAR10 dataset using multiple GPUs and analyzing the performance and scalability.\n",
    "\n",
    "## Overview\n",
    "The key steps involve setting up the DataLoader with CIFAR10 dataset transformations, training the model on different GPUs, measuring training time, analyzing scalability, and calculating communication bandwidth utilization.\n",
    "\n",
    "## Procedure\n",
    "- **Data Loading and Transformations**: Load CIFAR10 dataset with random cropping, horizontal flipping, and normalization transformations.\n",
    "- **Training Setup**: Implement ResNet-18 model training using DataParallel on multiple GPUs.\n",
    "- **Single GPU Training**: Measure training time for various batch sizes on a single GPU.\n",
    "- **Multi-GPU Training**: Measure training time and speedup on 2 and 4 GPUs, and analyze the type of scaling.\n",
    "- **Computation and Communication Time**: Calculate and report the time spent in computation and communication for multi-GPU setups.\n",
    "- **Bandwidth Utilization**: Calculate communication bandwidth utilization using the all-reduce algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ResNet in PyTorch.\n",
    "\n",
    "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
    "\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import warnings \n",
    "from IPython.display import clear_output\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = ResNet18()\n",
    "    y = net(torch.randn(1, 3, 32, 32))\n",
    "    print(y.size())\n",
    "\n",
    "# test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We are going to experiment with PyTorch’s DataParallel Module, which is PyTorch’s Synchronous SGD\n",
    " implementation across a number of GPUs on the same server. In particular, we will train ResNet-18 implementation from https://github.com/kuangliu/pytorch-cifar with num workers=2, running up to 4 GPUs\n",
    " with the DataParallel (DP) Module. Use SGD optimizers with 0.1 as the learning rate, momentum 0.9, and\n",
    " weight decay 5e-4. For this question, you need to do experiment with multiple GPUs on the same server.\n",
    " You may need to execute this on the NYU Greene Cluster.\n",
    " Create a PyTorch program with a DataLoader that loads the images and the related labels from the torchvision CIFAR10 dataset. Import the CIFAR10 dataset for the torchvision package, with the following sequence\n",
    "\n",
    " of transformations:\n",
    " - Random cropping, with size 32x32 and padding 4\n",
    " - Random horizontal flipping with a probability 0.5\n",
    " - Normalize each image’s RGB channel with mean(0.4914, 0.4822, 0.4465) and variance (0.2023, 0.1994,\n",
    " 0.2010)\n",
    "\n",
    " The DataLoader for the training set uses a minibatch size of 128 and 3 IO processes (i.e., num workers=2).\n",
    " The DataLoader for the testing set uses a minibatch size of 100 and 3 IO processes (i.e., num workers =2).\n",
    " Create a main function that creates the DataLoaders for the training set and the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms, datasets, models, utils\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "num_workers = 2\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(p=.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=np.sqrt([0.2023, 0.1994, .2010]))\n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data',\n",
    "                            train=True,\n",
    "                            download=True,\n",
    "                            transform=transform)\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data',\n",
    "                            train=False,\n",
    "                            download=True,\n",
    "                            transform=transform)\n",
    "\n",
    "\n",
    "trainloader = DataLoader(trainset,\n",
    "                         batch_size=128,\n",
    "                         shuffle=True,\n",
    "                         num_workers=num_workers)\n",
    "\n",
    "\n",
    "\n",
    "def load_cifar10():\n",
    "    trainloader = DataLoader(trainset,\n",
    "                             batch_size=128,\n",
    "                             shuffle=True,\n",
    "\n",
    "                             num_workers=num_workers)\n",
    "\n",
    "    testloader = DataLoader(testset,\n",
    "                            batch_size=100,\n",
    "                            shuffle=False,\n",
    "                            num_workers=num_workers)\n",
    "\n",
    "    return trainloader, testloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. Measure how long it takes to complete 1 epoch of training using different batch sizes on a single GPU.\n",
    " Start from batch size 32, increase by 4-fold for each measurement (i.e., 32, 128, 512 ...) until single\n",
    " GPU memory cannot hold the batch size. For each run, run 2 epochs, the first epoch is used to warm\n",
    " up the CPU/GPU cache; and you should report the training time (excluding data I/O; but including\n",
    " data movement from CPU to GPU, gradients calculation and weights update) based on the 2nd epoch\n",
    " training. (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:13<00:00, 113.45it/s]\n",
      "100%|██████████| 391/391 [00:07<00:00, 54.70it/s]\n",
      "100%|██████████| 98/98 [00:07<00:00, 13.95it/s]\n",
      "100%|██████████| 25/25 [00:07<00:00,  3.49it/s]\n",
      "100%|██████████| 7/7 [00:07<00:00,  1.04s/it]\n",
      "100%|██████████| 2/2 [00:10<00:00,  5.03s/it]\n",
      "100%|██████████| 1/1 [00:15<00:00, 15.27s/it]\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.62s/it]\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.59s/it]\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.64s/it]\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.76s/it]\n",
      "100%|██████████| 1/1 [00:15<00:00, 15.27s/it]\n",
      "100%|██████████| 1/1 [00:17<00:00, 17.20s/it]\n",
      "100%|██████████| 1/1 [00:26<00:00, 26.34s/it]\n",
      "100%|██████████| 1/1 [01:00<00:00, 60.39s/it]\n",
      "100%|██████████| 1/1 [03:16<00:00, 196.56s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import clear_output\n",
    "\n",
    "model = models.resnet18(num_classes=10)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "def train_one_epoch(loader):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for i, (inputs, labels) in enumerate(tqdm(loader), 0):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "results = []\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        trainloader = DataLoader(trainset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True,\n",
    "                                num_workers=num_workers)\n",
    "        time_taken = train_one_epoch(trainloader)\n",
    "        results.append((batch_size, time_taken))\n",
    "        batch_size *= 4\n",
    "    except:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Max GPU Memory reached at batch size {batch_size}\")\n",
    "        break\n",
    "\n",
    "tqdm.write(\"Results:\")\n",
    "print(f\"Max GPU Memory reached at batch size {batch_size}\")\n",
    "for batch_size, time_taken in tqdm(results):\n",
    "    tqdm.write(f\"Batch size: {str(batch_size):6s}, Time taken: {round(time_taken, 2):4s} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1\n",
    "**Note:** I ran the above code on a single NVIDIA A100 gpu with 40gb of memory such that GPU memory utilization never surpassed total GPU Memory. This is due to the single GPU having enough memory to house the entire epoch at once utilizing at most 83.3% GPU memory. \n",
    "\n",
    "```text\n",
    "Batch size: 32    , Time taken: 13.55 seconds\n",
    "Batch size: 128   , Time taken: 7.62  seconds\n",
    "Batch size: 512   , Time taken: 7.33  seconds\n",
    "Batch size: 2048  , Time taken: 7.57  seconds\n",
    "Batch size: 8192  , Time taken: 7.97  seconds\n",
    "Batch size: 32768 , Time taken: 10.38 seconds\n",
    "Batch size: 131072, Time taken: 15.27 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2. Measure running time with batch size per GPU you used in part 1 (i.e., 32, 128, ...) on 2 GPUs and\n",
    " 4 GPUs and calculate speedup for each setup. Again, for each setup, run 2 epochs, and only measure\n",
    " the 2nd epoch. When measuring speedup, one should include all the training components (e.g., data\n",
    " loading, cpu-gpu time, compute time). (5).\n",
    " Expected Answer: Table 1 records the training time and speedup for different batch sizes up to 4 GPUs.\n",
    " Comment on which type of scaling we are measuring: weak-scaling or strong-scaling? Comment on if\n",
    " the other type of scaling was used to speed up the number will be better or worse than what you are\n",
    " measuring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [01:08<00:00, 22.67it/s]\n",
      "100%|██████████| 1563/1563 [01:08<00:00, 22.85it/s]\n",
      "100%|██████████| 391/391 [00:17<00:00, 22.19it/s]\n",
      "100%|██████████| 391/391 [00:17<00:00, 22.22it/s]\n",
      "100%|██████████| 98/98 [00:07<00:00, 13.21it/s]\n",
      "100%|██████████| 98/98 [00:07<00:00, 13.36it/s]\n",
      "100%|██████████| 25/25 [00:07<00:00,  3.24it/s]\n",
      "100%|██████████| 25/25 [00:07<00:00,  3.30it/s]\n",
      "100%|██████████| 7/7 [00:07<00:00,  1.12s/it]\n",
      "100%|██████████| 7/7 [00:07<00:00,  1.13s/it]\n",
      "100%|██████████| 1563/1563 [01:25<00:00, 18.30it/s]\n",
      "100%|██████████| 1563/1563 [01:26<00:00, 18.04it/s]\n",
      "100%|██████████| 391/391 [00:21<00:00, 17.87it/s]\n",
      "100%|██████████| 391/391 [00:22<00:00, 17.75it/s]\n",
      "100%|██████████| 98/98 [00:07<00:00, 12.92it/s]\n",
      "100%|██████████| 98/98 [00:07<00:00, 13.18it/s]\n",
      "100%|██████████| 25/25 [00:07<00:00,  3.31it/s]\n",
      "100%|██████████| 25/25 [00:07<00:00,  3.34it/s]\n",
      "100%|██████████| 7/7 [00:07<00:00,  1.11s/it]\n",
      "100%|██████████| 7/7 [00:07<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speedup results for 2 GPUs:\n",
      "Batch size: 32,   Time taken: 68.39 seconds\n",
      "Batch size: 128,  Time taken: 17.6 seconds\n",
      "Batch size: 512,  Time taken: 7.34 seconds\n",
      "Batch size: 2048, Time taken: 7.58 seconds\n",
      "Batch size: 8192, Time taken:  7.9 seconds\n",
      "\n",
      "Speedup results for 4 GPUs:\n",
      "Batch size: 32,   Time taken: 86.65 seconds\n",
      "Batch size: 128,  Time taken: 22.03 seconds\n",
      "Batch size: 512,  Time taken: 7.44 seconds\n",
      "Batch size: 2048, Time taken: 7.49 seconds\n",
      "Batch size: 8192, Time taken: 7.83 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_one_epoch(loader, model, criterion, optimizer):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for i, (inputs, labels) in enumerate(tqdm(loader), 0):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "def measure_speedup(num_gpus, batch_sizes):\n",
    "    results = []\n",
    "    for batch_size in batch_sizes:\n",
    "        model = models.resnet18(num_classes=10)\n",
    "        if num_gpus > 1:\n",
    "            model = torch.nn.DataParallel(model, device_ids=list(range(num_gpus)))\n",
    "        model = model.to(device)\n",
    "\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "        trainloader = DataLoader(trainset,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=True,\n",
    "                                 num_workers=num_workers)\n",
    "\n",
    "        train_one_epoch(trainloader, model, criterion, optimizer)\n",
    "\n",
    "        time_taken = train_one_epoch(trainloader, model, criterion, optimizer)\n",
    "        results.append((batch_size, time_taken))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# Measure speedup for 2 and 4 GPUs\n",
    "batch_sizes = [32, 128, 512, 2048, 8192] \n",
    "\n",
    "speedup_2_gpus = measure_speedup(num_gpus=2, batch_sizes=batch_sizes)\n",
    "speedup_4_gpus = measure_speedup(num_gpus=4, batch_sizes=batch_sizes)\n",
    "\n",
    "\n",
    "print(\"Speedup results for 2 GPUs:\")\n",
    "for batch_size, time_taken in speedup_2_gpus:\n",
    "    print(f\"Batch size: {str(batch_size)+',':<5s} Time taken: {str(round(time_taken, 2)):>4s} seconds\")\n",
    "\n",
    "print(\"\\nSpeedup results for 4 GPUs:\")\n",
    "for batch_size, time_taken in speedup_4_gpus:\n",
    "    print(f\"Batch size: {str(batch_size)+',':<5s} Time taken: {str(round(time_taken, 2)):>4s} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2\n",
    "```text \n",
    "2 GPUs:\n",
    "Batch size: 32,   Time taken: 68.39 seconds\n",
    "Batch size: 128,  Time taken: 17.6  seconds\n",
    "Batch size: 512,  Time taken: 7.34  seconds\n",
    "Batch size: 2048, Time taken: 7.58  seconds\n",
    "Batch size: 8192, Time taken: 7.9   seconds\n",
    "4 GPUs:\n",
    "Batch size: 32,   Time taken: 86.65 seconds\n",
    "Batch size: 128,  Time taken: 22.03 seconds\n",
    "Batch size: 512,  Time taken: 7.44  seconds\n",
    "Batch size: 2048, Time taken: 7.49  seconds\n",
    "Batch size: 8192, Time taken: 7.83  seconds\n",
    "```\n",
    "Speedup Analysis:\n",
    "\n",
    "- Speedup was not linear due to overheads in communication and synchronization.\n",
    "    As the batch size increased, the speedup flattened or decreased due to overheads outweighing computation gains.\n",
    "    \n",
    "- This is an example of strong scaling as the problem size remains constant while GPUs are added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 4.3\n",
    "\n",
    "To calculate:\n",
    "Compute Time = Training time on 1 GPU \n",
    "Communication Time = Training time on N GPUs - Compute Time\n",
    "\n",
    "Results:\n",
    "\n",
    "| Batch Size | 2 GPU Compute | 2 GPU Comm | 4 GPU Compute | 4 GPU Comm |\n",
    "|------------|---------------|------------|---------------|------------|\n",
    "| 32         | 13.55         | 54.84      | 13.55         | 73.10      |\n",
    "| 128        | 7.62          | 9.98       | 7.62          | 14.41      | \n",
    "| 512        | 7.33          | 0.01       | 7.33          | 0.11       |\n",
    "| 2048       | 7.57          | 0.01       | 7.57          | -0.08      |\n",
    "| 8192       | 7.97          | -0.07      | 7.97          | -0.14      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch Size per GPU    2-GPU Bandwidth Utilization GB/s    4-GPU Bandwidth Utilization GB/s)\n",
      "--------------------  ----------------------------------  -----------------------------------\n",
      "                  32                          0.00127349                           0.00301536\n",
      "                 128                          0.00494850                           0.01186023\n",
      "                 512                          0.01186562                           0.03511841\n",
      "                2048                          0.01148993                           0.03488397\n",
      "                8192                          0.01102451                           0.03336922\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Constants\n",
    "num_parameters = 11689512  \n",
    "param_size_bytes = 4  \n",
    "\n",
    "# Calculating Allreduce Costs\n",
    "allreduce_cost_2_gpu_gb = 2 * (2 - 1) * num_parameters * param_size_bytes / 2**30\n",
    "allreduce_cost_4_gpu_gb = 2 * (4 - 1) * num_parameters * param_size_bytes / 2**30\n",
    "\n",
    "\n",
    "exec_times_2_gpu = {\n",
    "    32: 68.39,\n",
    "    128: 17.6,\n",
    "    512: 7.34,\n",
    "    2048: 7.58,\n",
    "    8192: 7.9\n",
    "}\n",
    "exec_times_4_gpu = {\n",
    "    32: 86.65,\n",
    "    128: 22.03,\n",
    "    512: 7.44,\n",
    "    2048: 7.49,\n",
    "    8192: 7.83\n",
    "}\n",
    "\n",
    "\n",
    "def calculate_bandwidth_utilization(allreduce_cost_gb, exec_time_sec):\n",
    "    return allreduce_cost_gb / exec_time_sec\n",
    "\n",
    "\n",
    "batch_sizes = [32, 128, 512, 2048, 8192]\n",
    "table_data = []\n",
    "for batch_size in batch_sizes:\n",
    "    bw_util_2_gpu = calculate_bandwidth_utilization(allreduce_cost_2_gpu_gb, exec_times_2_gpu[batch_size])\n",
    "    bw_util_4_gpu = calculate_bandwidth_utilization(allreduce_cost_4_gpu_gb, exec_times_4_gpu[batch_size])\n",
    "    table_data.append([batch_size, bw_util_2_gpu, bw_util_4_gpu])\n",
    "\n",
    "\n",
    "headers = [\"Batch Size per GPU\", \"2-GPU Bandwidth Utilization GB/s\", \"4-GPU Bandwidth Utilization GB/s)\"]\n",
    "print(tabulate(table_data, headers=headers, floatfmt=\".8f\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 \n",
    "\n",
    "\n",
    "| Formula | Description |\n",
    "|---------|-------------|\n",
    "| 2(N - 1) * K / N | All-reduce cost, where K is the number of model parameters and N is the number of GPUs |\n",
    "| Communication Time / All-reduce Cost | Bandwidth Utilization |\n",
    "\n",
    "Given:\n",
    "- Number of parameters in ResNet18 (K) = 11,689,512\n",
    "- 2 GPU All-reduce Cost = 2(2 - 1) * 11,689,512 = 11,689,512\n",
    "- 4 GPU All-reduce Cost = 2(4 - 1) * 11,689,512 = 17,534,268\n",
    "\n",
    "Converting All-reduce Costs to GB:\n",
    "- 2 GPU All-reduce Cost = 11,689,512 * 4 bytes / 2^30 = 0.044 GB\n",
    "- 4 GPU All-reduce Cost = 17,534,268 * 4 bytes / 2^30 = 0.065 GB\n",
    "\n",
    "\n",
    "```text\n",
    "  Batch Size per GPU    2-GPU Bandwidth Utilization GB/s    4-GPU Bandwidth Utilization GB/s\n",
    "--------------------  ----------------------------------  -----------------------------------\n",
    "                  32                          0.00127349                           0.00301536\n",
    "                 128                          0.00494850                           0.01186023\n",
    "                 512                          0.01186562                           0.03511841\n",
    "                2048                          0.01148993                           0.03488397\n",
    "                8192                          0.01102451                           0.03336922\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.043546825647354126, 0.06532023847103119)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allreduce_cost_2_gpu_gb/2, allreduce_cost_4_gpu_gb/4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
