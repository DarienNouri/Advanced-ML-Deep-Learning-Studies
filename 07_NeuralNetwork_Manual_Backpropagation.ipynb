{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Network Training and Backpropagation\n",
        "\n",
        "This notebook modifies a 2-layered neural network to a 3-layered neural network with scaled sigmoid activation functions. It includes forward propagation, cost calculation, and backpropagation with and without regularization.\n",
        "\n",
        "## Overview\n",
        "The key steps involve updating functions for scaled sigmoid, modifying forward propagation and cost calculation for additional layers, implementing backpropagation, and training the model.\n",
        "\n",
        "## Procedure\n",
        "- **Scaled Sigmoid Function**: Modify `sigmoid()` to return scaled sigmoid.\n",
        "- **Forward Propagation**: Update `forward_propagate()` for two hidden layers.\n",
        "- **Cost Calculation**: Update `cost()` for predictions from the 3-layered neural network with and without regularization.\n",
        "- **Sigmoid Gradient**: Modify `sigmoid_gradient()` to return gradient of scaled sigmoid function.\n",
        "- **Backpropagation**: Update `backprop()` to compute gradients and implement two versions, with and without regularization.\n",
        "- **Model Training**: Train the 3-layered neural network by minimizing the objective function.\n",
        "- **Model Accuracy**: Make forward predictions and compute accuracy.\n",
        "- **Comparison**: Compare model accuracy with the 2-layered neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "print('x')\n",
        "x=4\n",
        "x\n",
        "\n",
        "print(4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "Obko9XuCwZl1",
        "outputId": "defad257-cec1-4528-9834-0fa1d0d8b195"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct 16 13:09:09 2011',\n",
              " '__version__': '1.0',\n",
              " '__globals__': [],\n",
              " 'X': array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
              " 'y': array([[10],\n",
              "        [10],\n",
              "        [10],\n",
              "        ...,\n",
              "        [ 9],\n",
              "        [ 9],\n",
              "        [ 9]], dtype=uint8)}"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import loadmat\n",
        "%matplotlib inline\n",
        "\n",
        "data = loadmat('../data/ex3data1.mat')\n",
        "data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYex0IkBwZl2"
      },
      "source": [
        "Since we're going to need these later (and will use them often), let's create some useful variables up-front."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "3UdPhci_wZl2",
        "outputId": "07859bed-abcc-427b-adfb-968eaf0c168a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((5000, 400), (5000, 1))"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = data[\"X\"]\n",
        "y = data[\"y\"]\n",
        "\n",
        "X.shape, y.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMSgSTLrwZl2"
      },
      "source": [
        "We're also going to need to one-hot encode our y labels.  One-hot encoding turns a class label n (out of k classes) into a vector of length k where index n is \"hot\" (1) while the rest are zero.  Scikit-learn has a built in utility we can use for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "fC5Vj6U-wZl2",
        "outputId": "103c7774-9156-4a34-86fc-28d4cfde593d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5000, 10)"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_onehot = encoder.fit_transform(y)\n",
        "y_onehot.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "QR5Pp_u5wZl3",
        "outputId": "2e36a385-db7a-4e11-e490-fe040244a9ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([10], dtype=uint8), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]))"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y[0], y_onehot[0, :]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBW2_bxrwZl3"
      },
      "source": [
        "### OG Description\n",
        "The neural network we're going to build for this exercise has an input layer matching the size of our instance data (400 + the bias unit), a hidden layer with 25 units (26 with the bias unit), and an output layer with 10 units corresponding to our one-hot encoding for the class labels.  For additional details and an image of the network architecture, please refer to the PDF in the \"exercises\" folder.\n",
        "\n",
        "The first piece we need to implement is a cost function to evaluate the loss for a given set of network parameters.  The source mathematical function is in the exercise text (and looks pretty intimidating).  Here are the functions required to compute the cost.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Current Description\n",
        "\n",
        "Next, in the template provided, you will make a copy of this notebook and modify it to train a 3-layered neural network with two hidden layers using the same dataset. The number of hidden units in the first and second hidden layers is 20 and 20. The activation function you will use in hidden layers is scaled sigmoid, given by:\n",
        "\n",
        "$$ \\hat{\\sigma}(z) = \\frac{1}{1 + e^{-2z}} $$\n",
        "\n",
        "You will need to make changes to the following functions in the original notebook.\n",
        "\n",
        "1. `sigmoid()` to return scaled sigmoid. (1)\n",
        "2. `forward_propagate()` to account for 2 hidden layers (the original has one hidden layer). (3)\n",
        "3. `cost()` to calculate predictions from the 3-layered neural network and hence the cost. You need to make changes to both versions of the `cost()` function, with and without regularization, as in the original notebook. (4)\n",
        "4. `sigmoid_gradient()` to return gradient of scaled sigmoid function. (2)\n",
        "5. `backprop()` to compute the gradients. Your function should return both the cost and the gradient vector, as in the original notebook. Also, you will need to implement two versions of these functions, with and without regularization, as in the original notebook. (8)\n",
        "\n",
        "Then you will\n",
        "\n",
        "6. Train your 3-layered neural network by minimizing the objective function, as in the original notebook, keeping the hyperparameters (learning rate, method, `jac`, options) unchanged. (3)\n",
        "7. Make forward predictions from your trained model and compute the accuracy. (2)\n",
        "8. How does your model accuracy compare with the accuracy of the 2-layered neural network in the original notebook? (2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Custom Sigmoid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Math: \\sigma(z) = \\frac{1}{1 + e^{-2z}}\n",
        "\n",
        "def new_sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-2 * z))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Custom Propagate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "collapsed": true,
        "id": "Z49hEirrwZl4"
      },
      "outputs": [],
      "source": [
        "def new_forward_propagate(X, theta1, theta2, theta3):\n",
        "    m = X.shape[0]\n",
        "\n",
        "    a1 = np.insert(X, 0, values=np.ones(m), axis=1)\n",
        "\n",
        "    z2 = a1 * theta1.T\n",
        "    a2 = np.insert(new_sigmoid(z2), 0, values=np.ones(m), axis=1)\n",
        "\n",
        "    z3 = a2 * theta2.T\n",
        "    a3 = np.insert(new_sigmoid(z3), 0, values=np.ones(m), axis=1)\n",
        "\n",
        "    z4 = a3 * theta3.T\n",
        "    h = new_sigmoid(z4)\n",
        "\n",
        "    return a1, z2, a2, z3, a3, z4, h\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Custom Cost No Reg "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "def new_cost(params, input_size, hidden_size, num_labels, X, y, learning_rate):\n",
        "    m = X.shape[0]\n",
        "    X = np.matrix(X)\n",
        "    y = np.matrix(y)\n",
        "\n",
        "    # Reshape the parameter array into parameter matrices for each layer\n",
        "\n",
        "\n",
        "    theta1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
        "    theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):hidden_size * (input_size + 1) + hidden_size * (hidden_size + 1)], (hidden_size, (hidden_size + 1))))\n",
        "    theta3 = np.matrix(np.reshape(params[hidden_size * (input_size + 1) + hidden_size * (hidden_size + 1):], (num_labels, (hidden_size + 1))))\n",
        "\n",
        "\n",
        "    # Run the feed-forward pass\n",
        "    a1, z2, a2, z3, a3, z4, h = new_forward_propagate(X, theta1, theta2, theta3)\n",
        "\n",
        "    # Compute the cost\n",
        "    J = 0\n",
        "    for i in range(m):\n",
        "        first_term = np.multiply(-y[i, :], np.log(h[i, :]))\n",
        "        second_term = np.multiply((1 - y[i, :]), np.log(1 - h[i, :]))\n",
        "        J += np.sum(first_term - second_term)\n",
        "\n",
        "    J = J / m\n",
        "    return J\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkQmQ9NAwZl4"
      },
      "source": [
        "We've used the sigmoid function before so that's not new.  The forward-propagate function computes the hypothesis for each training instance given the current parameters.  It's output shape should match the same of our one-hot encoding for y.  We can test this real quick to convince ourselves that it's working as expected (the intermediate steps are also returned as these will be useful later)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sNU0MTAwZl4",
        "outputId": "0a150d4c-9e16-48ae-8330-f4b9adb90673"
      },
      "source": [
        "a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)\n",
        "\n",
        "a1.shape, z2.shape, a2.shape, z3.shape, h.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPZAdTzWwZl4"
      },
      "source": [
        "The cost function, after computing the hypothesis matrix h, applies the cost equation to compute the total error between y and h."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIP3-mnRwZl5",
        "outputId": "fbdb304c-388a-4ce3-a109-140b543bfd18"
      },
      "source": [
        "cost(params, input_size, hidden_size, num_labels, X, y_onehot, learning_rate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test new functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test for the new to train a 3-layered neural network with two hidden layers using the same dataset. The number of hidden units in the first and second hidden layers is 20 and 20. The activation function you will use in hidden layers is scaled sigmoid\n",
        "\n",
        "# initial setup\n",
        "input_size = 400\n",
        "hidden_size = 20\n",
        "num_labels = 10\n",
        "learning_rate = .9\n",
        "\n",
        "print(f\"==>> learning_rate: {learning_rate}\")\n",
        "\n",
        "# randomly initialize a parameter array of the size of the full network's parameters\n",
        "params = (\n",
        "    np.random.random(\n",
        "        size=hidden_size * (input_size + 1)\n",
        "        + hidden_size * (hidden_size + 1)\n",
        "        + num_labels * (hidden_size + 1)\n",
        "    )\n",
        "    - 0.5\n",
        ") * 0.25\n",
        "\n",
        "\n",
        "# params = (\n",
        "#     np.random.random(\n",
        "#         size=hidden_size * (input_size + 1) + num_labels * (hidden_size + 1)\n",
        "#     )\n",
        "#     - 0.5\n",
        "# ) * 0.25\n",
        "\n",
        "m = X.shape[0]\n",
        "X = np.matrix(X)\n",
        "y = np.matrix(y)\n",
        "\n",
        "# initialize theta 1, 2 and 3\n",
        "# theta1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
        "# theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):hidden_size * (input_size + 1) + hidden_size * (hidden_size + 1)], (hidden_size, (hidden_size + 1))))\n",
        "# theta3 = np.matrix(np.reshape(params[hidden_size * (input_size + 1) + hidden_size * (hidden_size + 1):], (num_labels, (hidden_size + 1))))\n",
        "# theta3 = np.matrix(np.reshape(params[hidden_size * (input_size + 1) + hidden_size * (hidden_size + 1):], (num_labels, (hidden_size + 1))))\n",
        "theta1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
        "theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):hidden_size * (input_size + 1) + hidden_size * (hidden_size + 1)], (hidden_size, (hidden_size + 1))))\n",
        "theta3 = np.matrix(np.reshape(params[hidden_size * (input_size + 1) + hidden_size * (hidden_size + 1):], (num_labels, (hidden_size + 1))))\n",
        "\n",
        "theta1.shape, theta2.shape, theta3.shape\n",
        "\n",
        "np.matrix(np.reshape(params[hidden_size * (input_size + 1) + hidden_size * (3 + 1):], (num_labels, (hidden_size + 1))))\n",
        "\n",
        "print(\"m shape:\", m)\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "print(\"theta1 shape:\", theta1.shape)\n",
        "print(\"theta2 shape:\", theta2.shape)\n",
        "\n",
        "print(\"params shape:\", params.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7.323056299376311"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a1, z2, a2, z3, a3, z4, h = new_forward_propagate(X, theta1, theta2, theta3)\n",
        "\n",
        "\n",
        "new_cost(params, input_size, hidden_size, num_labels, X, y_onehot, learning_rate )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHIVqWFQwZl5"
      },
      "source": [
        "Our next step is to add regularization to the cost function.  If you're following along in the exercise text and thought the last equation looked ugly, this one looks REALLY ugly.  It's actually not as complicated as it looks though - in fact, the regularization term is simply an addition to the cost we already computed.  Here's the revised cost function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Custom Cost with back propagation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "def custom_cost_reg(params, input_size, hidden_size, num_labels, X, y, learning_rate):\n",
        "    m = X.shape[0]\n",
        "    X = np.matrix(X)\n",
        "    y = np.matrix(y)\n",
        "\n",
        "    # Reshape the parameter array into parameter matrices for each layer\n",
        "    # Assuming the second hidden layer also has the same size as the first hidden layer\n",
        "    theta1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
        "    theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):hidden_size * (input_size + 1) + hidden_size * (hidden_size + 1)], (hidden_size, (hidden_size + 1))))\n",
        "    theta3 = np.matrix(np.reshape(params[hidden_size * (input_size + 1) + hidden_size * (hidden_size + 1):], (num_labels, (hidden_size + 1))))\n",
        "\n",
        "\n",
        "    # Run the feed-forward pass\n",
        "    a1, z2, a2, z3, a3, z4, h = new_forward_propagate(X, theta1, theta2, theta3)\n",
        "\n",
        "    # Compute the cost\n",
        "    J = 0\n",
        "    for i in range(m):\n",
        "        first_term = np.multiply(-y[i, :], np.log(h[i, :]))\n",
        "        second_term = np.multiply((1 - y[i, :]), np.log(1 - h[i, :]))\n",
        "        J += np.sum(first_term - second_term)\n",
        "\n",
        "\n",
        "    # with reg\n",
        "    J = J / m\n",
        "\n",
        "    # add the cost regularization term\n",
        "    J += (float(learning_rate) / (2 * m)) * (np.sum(np.power(theta1[:,1:], 2)) + np.sum(np.power(theta2[:,1:], 2)) + np.sum(np.power(theta3[:,1:], 2)))\n",
        "\n",
        "    return J\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRwMSeiSwZl5"
      },
      "source": [
        "Next up is the backpropagation algorithm.  Backpropagation computes the parameter updates that will reduce the error of the network on the training data.  The first thing we need is a function that computes the gradient of the sigmoid function we created earlier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Custom sigmoid_gradient\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "def new_sigmoid_gradient(z):\n",
        "    return np.multiply(new_sigmoid(z), (1 - new_sigmoid(z)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE2Ca3w-wZl5"
      },
      "source": [
        "Now we're ready to implement backpropagation to compute the gradients.  Since the computations required for backpropagation are a superset of those required in the cost function, we're actually going to extend the cost function to also perform backpropagation and return both the cost and the gradients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5 Custom Back Propagation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def new_backprop(params, input_size, hidden_size, num_labels, X, y, learning_rate):\n",
        "    m = X.shape[0]\n",
        "    X = np.matrix(X)\n",
        "    y = np.matrix(y)\n",
        "\n",
        "    # Reshape the parameter array into parameter matrices for each layer\n",
        "    # Assuming the second hidden layer also has the same size as the first hidden layer\n",
        "    theta1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
        "    theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):hidden_size * (input_size + 1) + hidden_size * (hidden_size + 1)], (hidden_size, (hidden_size + 1))))\n",
        "    theta3 = np.matrix(np.reshape(params[hidden_size * (input_size + 1) + hidden_size * (hidden_size + 1):], (num_labels, (hidden_size + 1))))\n",
        "\n",
        "    # Run the feed-forward pass\n",
        "    a1, z2, a2, z3, a3, z4, h = new_forward_propagate(X, theta1, theta2, theta3)\n",
        "\n",
        "    # Compute the cost\n",
        "    J = 0\n",
        "    for i in range(m):\n",
        "        first_term = np.multiply(-y[i, :], np.log(h[i, :]))\n",
        "        second_term = np.multiply((1 - y[i, :]), np.log(1 - h[i, :]))\n",
        "        J += np.sum(first_term - second_term)\n",
        "\n",
        "    # with reg\n",
        "    J = J / m\n",
        "\n",
        "    # add the cost regularization term\n",
        "    J += (float(learning_rate) / (2 * m)) * (np.sum(np.power(theta1[:,1:], 2)) + np.sum(np.power(theta2[:,1:], 2)) + np.sum(np.power(theta3[:,1:], 2)))\n",
        "\n",
        "    # perform backpropagation\n",
        "    delta1 = np.zeros(theta1.shape)  # (25, 401)\n",
        "    delta2 = np.zeros(theta2.shape)  # (25, 26)\n",
        "    delta3 = np.zeros(theta3.shape)  # (10, 26)\n",
        "\n",
        "    for t in range(m):\n",
        "        a1t = a1[t,:]  # (1, 401)\n",
        "        z2t = z2[t,:]  # (1, 25)\n",
        "        a2t = a2[t,:]  # (1, 26)\n",
        "        z3t = z3[t,:]  # (1, 25)\n",
        "        a3t = a3[t,:]  # (1, 26)\n",
        "        ht = h[t,:]  # (1, 10)\n",
        "        yt = y[t,:]  # (1, 10)\n",
        "\n",
        "        d4t = ht - yt  # (1, 10)\n",
        "\n",
        "        # Errors for the second hidden layer\n",
        "        z3t = np.insert(z3t, 0, values=np.ones(1))  # add bias unit\n",
        "        d3t = np.multiply((theta3.T * d4t.T).T, new_sigmoid_gradient(z3t))\n",
        "\n",
        "        # Errors for the first hidden layer\n",
        "        z2t = np.insert(z2t, 0, values=np.ones(1))  # add bias unit\n",
        "        d2t = np.multiply((theta2.T * d3t[:,1:].T).T, new_sigmoid_gradient(z2t))\n",
        "\n",
        "        delta1 = delta1 + (d2t[:,1:]).T * a1t\n",
        "        delta2 = delta2 + (d3t[:,1:]).T * a2t\n",
        "        delta3 = delta3 + d4t.T * a3t\n",
        "\n",
        "    delta1 = delta1 / m\n",
        "    delta2 = delta2 / m\n",
        "    delta3 = delta3 / m\n",
        "\n",
        "    # unravel the gradient matrices into a single array\n",
        "    grad = np.concatenate((np.ravel(delta1), np.ravel(delta2), np.ravel(delta3)))\n",
        "    return J, grad\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psm0xgmzwZl5"
      },
      "source": [
        "The hardest part of the backprop computation (other than understanding WHY we're doing all these calculations) is getting the matrix dimensions right.  By the way, if you find it confusing when to use A * B vs. np.multiply(A, B), you're not alone.  Basically the former is a matrix multiplication and the latter is an element-wise multiplication (unless A or B is a scalar value, in which case it doesn't matter).  I wish there was a more concise syntax for this (maybe there is and I'm just not aware of it).\n",
        "\n",
        "Anyway, let's test it out to make sure the function returns what we're expecting it to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "BgG6MnUiwZl5",
        "outputId": "444def72-f9ed-4150-c804-c80c8c2cb631"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7.327116265985744, (8650,))"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "J, grad = new_backprop(params, input_size, hidden_size, num_labels, X, y_onehot, learning_rate)\n",
        "J, grad.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54uqLJPlwZl5"
      },
      "source": [
        "We still have one more modification to make to the backprop function - adding regularization to the gradient calculations.  The final regularized version is below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "collapsed": true,
        "id": "eJ_I4JycwZl5"
      },
      "outputs": [],
      "source": [
        "def new_backprop_reg(params, input_size, hidden_size, num_labels, X, y, learning_rate):\n",
        "    m = X.shape[0]\n",
        "    X = np.matrix(X)\n",
        "    y = np.matrix(y)\n",
        "\n",
        "    # Reshape the parameter array into parameter matrices for each layer\n",
        "    # Assuming the second hidden layer also has the same size as the first hidden layer\n",
        "    theta1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
        "    theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):hidden_size * (input_size + 1) + hidden_size * (hidden_size + 1)], (hidden_size, (hidden_size + 1))))\n",
        "    theta3 = np.matrix(np.reshape(params[hidden_size * (input_size + 1) + hidden_size * (hidden_size + 1):], (num_labels, (hidden_size + 1))))\n",
        "\n",
        "    # Run the feed-forward pass\n",
        "    a1, z2, a2, z3, a3, z4, h = new_forward_propagate(X, theta1, theta2, theta3)\n",
        "\n",
        "    # Compute the cost\n",
        "    J = 0\n",
        "    for i in range(m):\n",
        "        first_term = np.multiply(-y[i, :], np.log(h[i, :]))\n",
        "        second_term = np.multiply((1 - y[i, :]), np.log(1 - h[i, :]))\n",
        "        J += np.sum(first_term - second_term)\n",
        "\n",
        "    # with reg\n",
        "    J = J / m\n",
        "\n",
        "    # add the cost regularization term\n",
        "    J += (float(learning_rate) / (2 * m)) * (np.sum(np.power(theta1[:,1:], 2)) + np.sum(np.power(theta2[:,1:], 2)) + np.sum(np.power(theta3[:,1:], 2)))\n",
        "\n",
        "    # perform backpropagation\n",
        "    delta1 = np.zeros(theta1.shape)  # (25, 401)\n",
        "    delta2 = np.zeros(theta2.shape)  # (25, 26)\n",
        "    delta3 = np.zeros(theta3.shape)  # (10, 26)\n",
        "\n",
        "    for t in range(m):\n",
        "        a1t = a1[t,:]  # (1, 401)\n",
        "        z2t = z2[t,:]  # (1, 25)\n",
        "        a2t = a2[t,:]  # (1, 26)\n",
        "        z3t = z3[t,:]  # (1, 25)\n",
        "        a3t = a3[t,:]  # (1, 26)\n",
        "        ht = h[t,:]  # (1, 10)\n",
        "        yt = y[t,:]  # (1, 10)\n",
        "\n",
        "        d4t = ht - yt  # (1, 10)\n",
        "\n",
        "        # Errors for the second hidden layer\n",
        "        z3t = np.insert(z3t, 0, values=np.ones(1))  # add bias unit\n",
        "        d3t = np.multiply((theta3.T * d4t.T).T, new_sigmoid_gradient(z3t))\n",
        "\n",
        "        # Errors for the first hidden layer\n",
        "        z2t = np.insert(z2t, 0, values=np.ones(1))  # add bias unit\n",
        "        d2t = np.multiply((theta2.T * d3t[:,1:].T).T, new_sigmoid_gradient(z2t))\n",
        "\n",
        "        delta1 = delta1 + (d2t[:,1:]).T * a1t\n",
        "        delta2 = delta2 + (d3t[:,1:]).T * a2t\n",
        "        delta3 = delta3 + d4t.T * a3t\n",
        "\n",
        "    delta1 = delta1 / m\n",
        "    delta2 = delta2 / m\n",
        "    delta3 = delta3 / m\n",
        "\n",
        "    # add the gradient regularization term\n",
        "    delta1[:,1:] = delta1[:,1:] + (theta1[:,1:] * learning_rate) / m\n",
        "    delta2[:,1:] = delta2[:,1:] + (theta2[:,1:] * learning_rate) / m\n",
        "    delta3[:,1:] = delta3[:,1:] + (theta3[:,1:] * learning_rate) / m\n",
        "\n",
        "    # unravel the gradient matrices into a single array\n",
        "    grad = np.concatenate((np.ravel(delta1), np.ravel(delta2), np.ravel(delta3)))\n",
        "    return J, grad\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "mTl_gVrawZl6",
        "outputId": "5e4f4ddc-91fa-4d93-b3a6-3928996c124f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7.161517852858925, (8650,))"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "J, grad = new_backprop_reg(params, input_size, hidden_size, num_labels, X, y_onehot, learning_rate)\n",
        "J, grad.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.6 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKMrf3kgwZl6"
      },
      "source": [
        "We're finally ready to train our network and use it to make predictions.  This is roughly similar to the previous exercise with multi-class logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "P6QRVd8iwZl6",
        "outputId": "99dcf6f3-43a3-4a73-d1f1-bf5b7409c65b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              " message: Max. number of function evaluations reached\n",
              " success: False\n",
              "  status: 3\n",
              "     fun: 0.18141891680252428\n",
              "       x: [ 7.168e-01  1.187e-02 ...  1.804e+00  1.016e+00]\n",
              "     nit: 26\n",
              "     jac: [ 4.088e-04  2.136e-06 ... -3.779e-04 -6.056e-04]\n",
              "    nfev: 251"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "# minimize the objective function\n",
        "fmin = minimize(fun=new_backprop, x0=params, args=(input_size, hidden_size, num_labels, X, y_onehot, learning_rate),\n",
        "                method='TNC', jac=True, options={'maxfun': 250})\n",
        "fmin\n",
        "\n",
        "fmin_reg = minimize(fun=new_backprop_reg, x0=params, args=(input_size, hidden_size, num_labels, X, y_onehot, learning_rate),\n",
        "                method='TNC', jac=True, options={'maxfun': 250})\n",
        "fmin_reg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmTD2CKYwZl6"
      },
      "source": [
        "We put a bound on the number of iterations since the objective function is not likely to completely converge.  Our total cost has dropped below 0.5 though so that's a good indicator that the algorithm is working.  Let's use the parameters it found and forward-propagate them through the network to get some predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.7\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "ldBo69GuwZl6",
        "outputId": "5f8f6be4-2469-4d1f-81e1-2c9c199496f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "minimize without regularization: 99.12%\n",
            "minimize with regularization: 99.14%\n"
          ]
        }
      ],
      "source": [
        "def calc_min(fmin, X=X, y=y, input_size=input_size, hidden_size=hidden_size, num_labels=num_labels):\n",
        "    X = np.matrix(X)\n",
        "\n",
        "    theta1 = np.matrix(np.reshape(fmin.x[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
        "    theta2 = np.matrix(np.reshape(fmin.x[hidden_size * (input_size + 1):hidden_size * (input_size + 1) + hidden_size * (hidden_size + 1)], (hidden_size, (hidden_size + 1))))\n",
        "    theta3 = np.matrix(np.reshape(fmin.x[hidden_size * (input_size + 1) + hidden_size * (hidden_size + 1):], (num_labels, (hidden_size + 1))))\n",
        "\n",
        "    a1, z2, a2, z3, a3, z4, h = new_forward_propagate(X, theta1, theta2, theta3)\n",
        "\n",
        "    y_pred = np.array(np.argmax(h, axis=1) + 1)\n",
        "\n",
        "    correct = [1 if a == b else 0 for (a, b) in zip(y_pred, y)]\n",
        "    accuracy = (sum(map(int, correct)) / float(len(correct)))\n",
        "\n",
        "    # print('accuracy = {0}%'.format(accuracy * 100))\n",
        "\n",
        "    return accuracy * 100\n",
        "\n",
        "print(f'minimize without regularization: {calc_min(fmin):.2f}%')\n",
        "print(f'minimize with regularization: {calc_min(fmin_reg):.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accuracy Comparison between 2 and 3 hidden layered Neural Network\n",
        "\n",
        "\n",
        "The 3-Layered Network produced the folowing accuracies:\n",
        "* minimize without regularization: 99.68%\n",
        "* minimize with regularization: 99.72%\n",
        "  \n",
        "The 2-Layered Network produced the following accuracies:\n",
        "* minimize with regularization: 99.22%\n",
        "  \n",
        "The 3-Layered Network is more accurate than the 2-Layered Network. However, the difference is not that significant. It is only 0.46% more accurate. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy = 0.88%\n"
          ]
        }
      ],
      "source": [
        "X = np.matrix(X)\n",
        "\n",
        "theta1 = np.matrix(np.reshape(fmin.x[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
        "theta2 = np.matrix(np.reshape(fmin.x[hidden_size * (input_size + 1):hidden_size * (input_size + 1) + hidden_size * (hidden_size + 1)], (hidden_size, (hidden_size + 1))))\n",
        "theta3 = np.matrix(np.reshape(fmin.x[hidden_size * (input_size + 1) + hidden_size * (hidden_size + 1):], (num_labels, (hidden_size + 1))))\n",
        "a1, z2, a2, z3, a3, z4, h = new_forward_propagate(X, theta1, theta2, theta3)\n",
        "\n",
        "y_pred = np.array(np.argmax(h, axis=1) + 1)\n",
        "\n",
        "\n",
        "correct = [0 if a == b else 1 for (a, b) in zip(y_pred, y)]\n",
        "accuracy = (sum(map(int, correct)) / float(len(correct)))\n",
        "print('accuracy = {0}%'.format(accuracy * 100))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_TV0zbDwZl6"
      },
      "source": [
        "Finally we can compute the accuracy to see how well our trained network is doing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.8\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "wfnHyZZzwZl6",
        "outputId": "4ba1a64b-a1ca-462c-a528-ddf2eee9dc20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy = 0.88%\n"
          ]
        }
      ],
      "source": [
        "correct = [0 if a == b else 1 for (a, b) in zip(y_pred, y)]\n",
        "accuracy = (sum(map(int, correct)) / float(len(correct)))\n",
        "print('accuracy = {0}%'.format(accuracy * 100))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpj20whPwZl6"
      },
      "source": [
        "And we're done!  We've successfully implemented a rudimentary feed-forward neural network with backpropagation and used it to classify images of handwritten digits.  In the next exercise we'll look at another power supervised learning algorithm, support vector machines."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "EnvML",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
